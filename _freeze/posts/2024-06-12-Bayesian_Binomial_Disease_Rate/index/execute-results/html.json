{
  "hash": "e31ced86da6171f2973753eca353a8da",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Inference Case Study: Disease Prevalence\"\ndescription: \"tbd\"\nauthor:\n  - name: Net Zhang\n    url: https://netzissou.github.io/blog/\ndate: 06-12-2024\ncategories: [Bayesian] # self-defined categories\nimage: cover.jpg\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n# Intro\n\nIn Bayesian statistics, we use a powerful framework to update our beliefs based on new evidence. Unlike traditional frequentist methods, Bayesian approaches take into account prior information and produce a posterior distribution that reflects updated beliefs after observing the data. In this tutorial, we will walk through a simple Bayesian modeling process using regional disease testing as a case study.\n\n**What is this post about?**\n\nThis is part of my Bayesian Inference series, where I present problems (mostly in public health settings) and apply Bayesian approaches.\n\n**Who is this for? And what's the motivation?**\n\nI've been working as a data scientist at a higher institution for the past 2 years, supporting data operations from public health authorities and researchers. However, I've noticed that my job has started to shift more towards data management and software engineering but I don't want my modeling skills to get rusty. Working at a university has its perks, such as many opportunities to teach, and I've learned a lot from participating in workshops. I believe that mastering knowledge doesn't always mean being able to pass it on. For this reason, I want to create theory-focused tutorials with R or Python scripts that follow good software engineering principles. I want to help people, including myself, to master the theory behind basic Bayesian models. These tutorials are aimed at those who are passionate about Bayesian Inference and would like to refresh their knowledge of Bayesian 101 Modeling.\n\n# Case Study\n\nImagine a public health department conducting tests for a contagious disease in a specific community. Their goal is to estimate the prevalence of the disease in this community, which represents the probability that an individual in the community has the disease.\n\n# Assumptions\n\nTo apply the Bayesian method, we start with a set of assumptions:\n\n1.  Binary Outcomes: Each test result is either positive or negative.\n\n2.  Independence: Each test result is assumed to be independent of others.\n\n3.  Identical Distribution: Each individual in the population has the same probability of testing positive.\n\n# Build A Model\n\nGiven these assumptions, we can treat the tests as independent and identically distributed random variables $x_i$ following the Bernoulli distribution. The total number of positive cases $X$ is a random variable that follows the binomial distribution.\n\n$$\nx_i \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta)\n$$\n\n$$\nX \\sim \\text{Binomial}(n, \\theta)\n$$\n\nAt this point, we have already specified our model. Now is the time to ask ourselves what kind of questions we wish our model to answer. In the public health setting, we are interested in questions like\n\n-   What is the estimated prevalence of the disease in the community?\n\n-   How confident are we in our estimate of the disease prevalence?\n\n-   How does new data (e.g., additional test results) update our beliefs about disease prevalence?\n\n-   What is the probability that the prevalence exceeds a certain threshold (e.g., 5%)?\n\n-   How do different prior beliefs about the prevalence affect our posterior estimates?\n\n-   Given our current estimates, what is the expected number of positive cases in the next 50 tests?\n\n-   How reliable are our predictions about the number of positive cases in the upcoming tests?\n\nIn summary, the typical expectation is to deliver an estimate and communicate the level of confidence in that estimation. You'll see that once we establish the statistical distribution, addressing these questions will become more straightforward.\n\n# Distributions\n\nLet's derive these distribution functions based on our specified binomial data model. We'll use these functions to answer the questions listed above.\n\n## Likelihood Function\n\nThe likelihood function represents the probability of observing the data given a specific value of the parameter. Since the random variable $X$ follows a binomial distribution, we can write the likelihood as a function of the data given the value of parameters as below:\n\n$$\nf(X = x | \\theta) = \\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n$$\n\n## Prior Distribution\n\nThe prior distribution represents our initial beliefs about the parameter before any data is observed. The Beta distribution is often chosen as the prior distribution in Bayesian modeling for parameters that represent probabilities.\n\n$$\n\\theta \\sim \\text{Beta}(\\alpha, \\beta) \n$$\n\nSince Beta distribution is a known distribution, we could write out its probability density function as follows:\n\n$$\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\cdot \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\n$$\n\n$$\n\\Gamma(\\alpha) = (\\alpha - 1)!\n$$\n\n$$\n B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n$$\n\n**Why choose Beta as our prior?**\n\nHere, I will briefly explain why we want to use Beta distribution as our prior in this example:\n\n1.  Flexibility & Interpretability\n\nThe beta distribution is highly flexible and can take on a variety of shapes depending on its two parameters, $\\alpha$ and $\\beta$. This flexibility allows it to represent a wide range of prior beliefs about the probability parameter, in our case, the case rate.\n\nThe parameters of the Beta distribution have intuitive interpretations. $\\alpha$ can be thought of as the number of prior successes, and $\\beta$ is the number of prior failures. This makes it easy to incorporate prior knowledge or expert opinions. When we have complete uncertainty of the parameters, we assign the prior to $Beta(1,1)$, which is equivalent to a uniform distribution.\n\n2.  Conjugate Prior for the Binomial Distribution\n\n**What is conjugacy?** The Beta distribution is the conjugate prior to the Binomial distribution. This means that when a Beta prior is combined with a Binomial likelihood, the posterior distribution is also a Beta distribution. You can see that this property greatly simplifies the Bayesian updating process when we derive the posterior for the unknown parameters.\n\n**How do we translate expert knowledge into hyper-parameters?**\n\nGenerally, your expert team will provide an expected estimate and their confidence level.\n\nWe could derive the expectation and standard deviation from the Beta distribution likelihood.\n\n$$\nE(\\theta)=\\frac{\\alpha}{\\alpha + \\beta}\n$$\n\nThe concentration of the Beta distribution is determined by $\\alpha+\\beta$ A higher sum means more confidence.\n\nIn this case study, suppose our public health workers have an estimate of the prevalence rate to be 20% before we roll out the community testing. They have moderate confidence, similar to having results from about 50 prior tests with 10 positives. Therefore, we could set $\\alpha$ to 10 and $\\beta$ to 40.\n\n## Posterior Distribution\n\nThe posterior distribution combines the prior distribution and the likelihood function using Bayes' theorem. It represents our updated beliefs about the parameter after observing the data.\n\n$$\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{f(x)}\n$$\n\n$$\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta}\n$$\n\nThe denominator $\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta$ is the normalizing constant ensuring that the posterior distribution integrates to 1. We can see that the posterior distribution is proportional to the product of the prior and the likelihood\n\n$$\nf(\\theta | x) \\propto f(x | \\theta) \\cdot f(\\theta)\n$$\n\n$$\nf(\\theta | x) \\propto \\theta^{\\alpha + x - 1} \\cdot (1 - \\theta)^{\\beta + n - x - 1} \\\n$$\n\n$$\nf(\\theta | x) \\propto \\text{Beta}(\\alpha + x, \\beta + n - x)\n$$\n\n$$\n\\mathbb{E}(\\theta | x) = \\frac{\\alpha + x}{\\alpha + \\beta + n}\n$$\n\n## Predictive Distribution\n\nThe predictive distribution allows us to make predictions about future observations based on the posterior distribution. It incorporates the uncertainty in the parameter estimate.\n\nFrom the previous notes, we have the prior PDF and likelihood function\n\n$$\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\cdot \\theta^{x} (1 - \\theta)^{n - x}\n$$\n\n$$\nf(x|\\theta)=\\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n$$\n\nAnd we could derive the predictive distribution using the prior and likelihood:\n\n$$\nf(x)=\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta\n$$\n\n$$\nf(x)=\\binom{n}{x}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\frac{\\Gamma(\\alpha + x)\\Gamma(n+\\beta-x)}{\\Gamma(\\alpha+\\beta+n)}=\\binom{n}{x}\\frac{B(\\alpha+x, n+\\beta-x)}{B(\\alpha, \\beta)}\n$$\n\nGiven a Beta prior, the predictive distribution is a Beta-Binomial Distribution modeling the number of successes in ùëõ trials when the probability of success $\\theta$ is uncertain.\n\n$$\nP(X=x|\\alpha, \\beta,n) = \\binom{n}{x}\\frac{B(\\alpha+x, n+\\beta-x)}{B(\\alpha, \\beta)}\n$$\n\nThe expected value of the Beta-binomial distribution is given by:\n\n$$\n\\mathbb{E}[X|\\alpha, \\beta,n] = \\mathbb{E}[n*p] = n \\cdot \\frac{\\alpha}{\\alpha + \\beta}\n$$\n\n# Answering Questions\n\nNow, I'm pretty confident that we're equipped with all the tools we need to answer those questions. Our testing team just got back with the first round of testing data: 100 individuals were tested, and 22 tested positive.\n\n## Estimating Disease Prevalence\n\n**What is the estimated prevalence of the disease in the community?**\n\n**How confident are we in our estimate of the disease prevalence?**\n\n**What is the probability that the prevalence exceeds a certain threshold (e.g., 5%)?**\n\nThese are questions about our estimation of the unknown parameters. We could answer these questions using the posterior distribution's probability density function.\n\nFirst, plug in the data and hyper-parameters into the posterior function we get\n\n$$\nf(\\theta | x) \\propto \\text{Beta}(32, 128)\n$$\n\nThen let's plot out the distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\n\n# Function to calculate posterior parameters\nget_post_params <- function(data, prior) {\n  post_alpha <- prior$alpha + data$x\n  post_beta <- prior$beta + data$n - data$x\n  return(list(alpha = post_alpha, beta = post_beta))\n}\n\n# Function to generate posterior data for plotting\ngenerate_post_pdf_tbl <- function(post_params, label) {\n  tibble(\n    theta = seq(0, 1, length.out = 5000),\n    posterior_density = dbeta(theta, post_params$alpha, post_params$beta),\n    round = label\n  )\n}\n\n# Function to calculate credible interval and mean\nget_credible_interval_and_mean <- function(post_params) {\n  credible_interval <- qbeta(c(0.025, 0.975), post_params$alpha, post_params$beta)\n  mean_posterior <- post_params$alpha / (post_params$alpha + post_params$beta)\n  return(list(credible_interval = credible_interval, mean_posterior = mean_posterior))\n}\n\n# Data for the first round\ndata_round_1 <- list(n = 100, x = 22)\nprior_params <- list(alpha = 10, beta = 40)\n\n# Calculate posterior for the first round\npost_params_1 <- get_post_params(data_round_1, prior_params)\npost_pdf_tbl_1 <- generate_post_pdf_tbl(post_params_1, \"Posterior after Round 1\")\ncredible_info_1 <- get_credible_interval_and_mean(post_params_1)\n\n# Plot the posterior distribution for the first round\nggplot(post_pdf_tbl_1, aes(x = theta, y = posterior_density)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = credible_info_1$mean_posterior, linetype = \"dashed\", color = \"red\") +\n  geom_ribbon(data = filter(post_pdf_tbl_1, theta >= credible_info_1$credible_interval[1] & theta <= credible_info_1$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"blue\", alpha = 0.2) +\n  labs(title = \"Posterior Distribution after Round 1\",\n       x = \"Disease Prevalence (Theta)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = credible_info_1$mean_posterior, y = max(post_pdf_tbl_1$posterior_density) * 0.9, label = paste(\"Mean:\", round(credible_info_1$mean_posterior, 3)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nBased on our posterior, we estimate the disease prevalence with the posterior expectation value $E(\\theta|X=x)=0.213$. Our confidence is reflected by the spread of the posterior distribution. The standard deviation and the 95% credible interval quantify this confidence.\n\n## Updating Beliefs with New Data\n\nOne of the core strengths of Bayesian inference lies in its ability to update our beliefs as new data becomes available. Now, suppose we received a new round of testing results: 100 individuals were tested, and 8 tested positive.\n\nOur posterior from the last round now becomes the prior, and we'll use the new test data to update the posterior:\n\n$$\nf(\\theta | x) \\propto \\text{Beta}(32 + x, 128 + n - x)\n$$\n\n$$\nf(\\theta | x) \\propto \\text{Beta}(40, 220)\n$$\n\nA pretty easy process. And let's visualize the updated posterior function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Given posterior for the second round\npost_params_2 <- list(alpha = 40, beta = 220)\npost_pdf_tbl_2 <- generate_post_pdf_tbl(post_params_2, \"Posterior after Round 2\")\ncredible_info_2 <- get_credible_interval_and_mean(post_params_2)\n\n# Combine both posteriors into one data frame\npost_pdf_tbl <- bind_rows(post_pdf_tbl_1, post_pdf_tbl_2)\n\n# Plot both posterior distributions\nggplot(post_pdf_tbl, aes(x = theta, y = posterior_density, color = round, fill = round)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = credible_info_1$mean_posterior, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = credible_info_2$mean_posterior, linetype = \"dashed\", color = \"blue\") +\n  geom_ribbon(data = filter(post_pdf_tbl_1, theta >= credible_info_1$credible_interval[1] & theta <= credible_info_1$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"blue\", alpha = 0.2) +\n  geom_ribbon(data = filter(post_pdf_tbl_2, theta >= credible_info_2$credible_interval[1] & theta <= credible_info_2$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"green\", alpha = 0.2) +\n  labs(title = \"Posterior Distributions of Disease Prevalence\",\n       x = \"Disease Prevalence (Theta)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = credible_info_1$mean_posterior, y = max(post_pdf_tbl_1$posterior_density) * 0.9, label = paste(\"Mean Round 1:\", round(credible_info_1$mean_posterior, 3)), color = \"red\") +\n  annotate(\"text\", x = credible_info_2$mean_posterior, y = max(post_pdf_tbl_2$posterior_density) * 0.9, label = paste(\"Mean Round 2:\", round(credible_info_2$mean_posterior, 3)), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Predictive Analysis\n\n**Given our current estimates, what is the expected number of positive cases in the next 50 tests?**\n\n**How reliable are our predictions about the number of positive cases in the upcoming tests?**\n\nWe need to have the predictive distribution at hand to answer these types of questions. And luckily, from the previous section, we've already proven that the predictive distribution follows a Beta-Binomial distribution, which is a function of the data given the prior.\n\n$$\nP(X=x|\\alpha_{post}, \\beta_{post},n) = \\binom{n}{x}\\frac{B(\\alpha_{post}+x, n+\\beta_{post}-x)}{B(\\alpha_{post}, \\beta_{post})}\n$$\n\nBased on two rounds of testing data and the question statement, we have $\\alpha_{post} = 40$, $\\beta_{post} = 220$, and $n=50$.\n\nThere is currently no base R function for beta-binomial distribution. However, you could try implementing it independently since we already figured out the math.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate Beta-binomial probability\ndbeta_binom <- function(x, n, alpha, beta) {\n  choose(n, x) * beta(alpha + x, beta + n - x) / beta(alpha, beta)\n}\n\n# Function to calculate Beta-binomial quantiles\nqbeta_binom <- function(p, n, alpha, beta) {\n  sapply(p, function(prob) {\n    q <- 0\n    cumulative_prob <- 0\n    while (cumulative_prob < prob) {\n      cumulative_prob <- cumulative_prob + dbeta_binom(q, n, alpha, beta)\n      q <- q + 1\n    }\n    return(q - 1)\n  })\n}\n```\n:::\n\n\nNow let's try to visualize the predictive distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictive distribution parameters\nn_future <- 50  # Number of future trials\nalpha_post <- post_params_2$alpha\nbeta_post <- post_params_2$beta\n\n# Generate the predictive distribution using Beta-binomial\npred_x <- 0:n_future\npred_density <- sapply(pred_x, function(x) dbeta_binom(x, n_future, alpha_post, beta_post))\n\n# Calculate the 95% predictive interval for the predictive distribution\npredictive_interval <- qbeta_binom(c(0.025, 0.975), n_future, alpha_post, beta_post)\n\n# Create a data frame for the predictive distribution\npred_df <- tibble(\n  x = pred_x,\n  density = pred_density\n)\n\n# Plot the predictive distribution\nggplot(pred_df, aes(x = x, y = density)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_errorbar(aes(x = predictive_interval[1], ymin = 0, ymax = max(density)), linetype = \"dotted\", color = \"blue\", width = 0.2) +\n  geom_errorbar(aes(x = predictive_interval[2], ymin = 0, ymax = max(density)), linetype = \"dotted\", color = \"blue\", width = 0.2) +\n  labs(title = \"Predictive Distribution of Future Positive Tests\",\n       x = \"Number of Positive Tests\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  # annotate(\"text\", x = alpha_post / (alpha_post + beta_post) * n_future, y = max(density) * 0.9, \n  #          label = paste(\"Mean:\", round(alpha_post / (alpha_post + beta_post) * n_future, 2)), color = \"red\") +\n  geom_ribbon(data = filter(pred_df, x >= predictive_interval[1] & x <= predictive_interval[2]), \n              aes(ymax = density), ymin = 0, fill = \"blue\", alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nTherefore, given our current estimates, we expect approximately 8 positive cases in the next 50 tests. Based on the 95% predictive interval (approximately 4 to 12), our predictions are reliable within this range, indicating that while there's some uncertainty, we have a high probability that the true count will fall within these bounds.\n\n# Sensitivity Analysis\n\nTBD\n\n# Questioning the Assumptions\n\nNow if you are already pretty deep into the Bayesian inferences world, you might notice that we've greatly simplified this problem to a coin-toss problem. Our peers could challenge our model by questioning our assumptions.\n\nOur i.i.d. assumption states that each test result (whether positive or negative) is independent of the others and that each test has the same probability $\\theta$ of being positive (identically distributed). This means the probability of a positive test is the same for every individual tested and that one person's test result does not influence another's.\n\nHowever, there could be some potential issues with the i.i.d assumptions in public health testing. I'll enumerate some of them and briefly discuss how to address them.\n\n## Cluster Infections\n\nIn a real-world scenario, infections might not be evenly distributed across the population. There could be clusters of infections due to localized outbreaks or hot-pots, leading to correlations between test results.\n\nTo address this, we should begin with identify and model clusters within the data. For example, use hierarchical models that allow for different prevalence rates in different subgroups or locations.\n\n## Testing Bias\n\nThe way individuals are selected for testing might not be random. For example, people with symptoms or known exposure to the disease might be more likely to get tested, which can skew the results.\n\nIf you are from the ML world, you already know what to do. We could include covariates that account for factors affecting the probability of a positive test, such as symptoms, exposure history, or demographic factors.\n\n## Temporal Changes\n\nAs the title suggested, the prevalence of the disease might change over time due to factors such as public health interventions, changes in behavior, or natural progression of the outbreak. This means the probability of a positive test could vary over time. We should consider using time series models or Bayesian hierarchical models that account for changes in prevalence over time.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}