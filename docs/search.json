[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Net Zhang",
    "section": "",
    "text": "Welcome\n&lt;– This is Net. Net’s profession is working with data, and he has been an R enthusiast for a long time. He is a firm believer in continuous learning. Net also believes that relationships are the keys to everything in his life.\nAside from work, Net enjoys playing guitar, exploring blues, funk, jazz, folk, world music, and world cinema. He is a non-competitive trail runner. Salomon and Saucony are his brands of choice, but he plans to try some HOKA soon.\n\n\nEducation\nBS in Computational Data Analytics (2022) The Ohio State University\n\n\nExperience\nData Scientist (2022 - present) The Ohio State University College of Public Health / Translational Data Analytics Institute\nCommercial Lines Pricing Analyst (2022) Progressive Insurance\nCorporate Advanced Analyst (2021 - 2022) Worthington Industries\nUndergraduate Research Developer (2020-2021) Battelle Center for Science, Engineering and Public Policy"
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html",
    "href": "posts/2023-10-31-learning_curves/index.html",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "",
    "text": "Typically, the x-axis represents the experience, which could be the number of training examples, the number of iterations, or the amount of time spent training, while the y-axis represents the performance, which could be accuracy, error rate, or another relevant metric."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#convergence",
    "href": "posts/2023-10-31-learning_curves/index.html#convergence",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "1. Convergence",
    "text": "1. Convergence\n\nDefinition: Convergence refers to the point at which the performance of the model stabilizes, and additional training brings negligible improvement.\nWhere to Look: Focus on whether the training and validation curves are leveling off and reaching a plateau.\nFollow-up Actions:\n\nIf the curves have not converged, consider increasing the number of training epochs or adjusting the learning rate.\nIf the curves have converged to a suboptimal performance, consider increasing model complexity or improving feature engineering."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#consistency",
    "href": "posts/2023-10-31-learning_curves/index.html#consistency",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "2. Consistency",
    "text": "2. Consistency\n\nDefinition: Consistency refers to the stability and reliability of the learning process over time.\nWhere to Look: Look for erratic fluctuations or high variance in the learning curves.\nFollow-up Actions:\n\nIf the curves are inconsistent, try using a smaller learning rate, different optimization algorithms, or regularization techniques.\nEnsure that the data is properly preprocessed and that the model is not too sensitive to the initialization."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#gap-between-training-and-validation-curves",
    "href": "posts/2023-10-31-learning_curves/index.html#gap-between-training-and-validation-curves",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "3. Gap Between Training and Validation Curves",
    "text": "3. Gap Between Training and Validation Curves\n\nDefinition: The gap between the training and validation curves indicates the level of overfitting or underfitting.\nWhere to Look: Focus on the final gap between the curves once they have converged.\nFollow-up Actions:\n\nA large gap (with high training performance and low validation performance) indicates overfitting. To address this, you can add more data, simplify the model, or increase regularization.\nA small gap with poor performance on both sets indicates underfitting. In this case, consider increasing model complexity, reducing regularization, or improving feature engineering."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#rate-of-learning",
    "href": "posts/2023-10-31-learning_curves/index.html#rate-of-learning",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "4. Rate of Learning",
    "text": "4. Rate of Learning\n\nDefinition: The rate at which the performance improves over time.\nWhere to Look: Look at the slope of the learning curves during training.\nFollow-up Actions:\n\nA slow rate of learning might indicate a small learning rate or poor feature scaling. Consider adjusting the learning rate or preprocessing the features.\nA fast rate of learning that suddenly plateaus might indicate that the model has quickly found a local minimum. Experiment with different initialization methods or learning rates."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#final-performance-level",
    "href": "posts/2023-10-31-learning_curves/index.html#final-performance-level",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "5. Final Performance Level",
    "text": "5. Final Performance Level\n\nDefinition: The level of performance that the model has achieved after training.\nWhere to Look: Look at the final value of the validation curve.\nFollow-up Actions:\n\nIf the final performance is not satisfactory, consider revisiting the model selection, feature engineering, or other aspects of the training process."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#high-bias-underfitting",
    "href": "posts/2023-10-31-learning_curves/index.html#high-bias-underfitting",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "1. High Bias (Underfitting)",
    "text": "1. High Bias (Underfitting)\n\nDiagnosis: The learning curves for both the training and validation sets plateau at a low level of performance.\nFollow-up Actions:\n\nIncrease model complexity (e.g., use a more complex model, add polynomial features).\nDecrease regularization.\nIncrease the number of features or improve feature selection.\nTrain for a longer time if the model has not yet converged."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#high-variance-overfitting",
    "href": "posts/2023-10-31-learning_curves/index.html#high-variance-overfitting",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "2. High Variance (Overfitting)",
    "text": "2. High Variance (Overfitting)\n\nDiagnosis: The training curve shows high performance, but the validation curve plateaus at a significantly lower level of performance.\nFollow-up Actions:\n\nIncrease the amount of training data.\nDecrease model complexity (e.g., use a simpler model, reduce the number of features).\nIncrease regularization.\nImprove data quality (e.g., remove noisy examples, correct mislabelings).\nImplement data augmentation (if applicable)."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#good-fit",
    "href": "posts/2023-10-31-learning_curves/index.html#good-fit",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "3. Good Fit",
    "text": "3. Good Fit\n\nDiagnosis: The learning curves for both training and validation sets plateau at a high level of performance, and the gap between them is small.\nFollow-up Actions:\n\nConsider further tuning hyperparameters to see if performance can be slightly improved.\nExplore ensemble methods to potentially boost performance.\nDeploy the model, but continue to monitor its performance on new data."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#overtraining",
    "href": "posts/2023-10-31-learning_curves/index.html#overtraining",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "4. Overtraining",
    "text": "4. Overtraining\n\nDiagnosis: Initially, both training and validation performance improve, but after a certain point, the validation performance starts to degrade while training performance continues to improve.\nFollow-up Actions:\n\nImplement early stopping to halt training when validation performance begins to degrade.\nIncrease regularization.\nReduce the learning rate."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#insufficient-training",
    "href": "posts/2023-10-31-learning_curves/index.html#insufficient-training",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "5. Insufficient Training",
    "text": "5. Insufficient Training\n\nDiagnosis: The learning curves show steady improvement, but they have not yet plateaued, indicating that the model could benefit from further training.\nFollow-up Actions:\n\nContinue training for more epochs or iterations.\nEnsure that the learning rate is not set too low, which could slow down training."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#learning-rate-issues",
    "href": "posts/2023-10-31-learning_curves/index.html#learning-rate-issues",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "6. Learning Rate Issues",
    "text": "6. Learning Rate Issues\n\nDiagnosis:\n\nIf the learning curve is very erratic, the learning rate might be too high.\nIf the learning curve shows very slow improvement, the learning rate might be too low.\n\nFollow-up Actions:\n\nAdjust the learning rate accordingly.\nImplement learning rate scheduling to decrease the learning rate over time."
  },
  {
    "objectID": "posts/2023-10-31-learning_curves/index.html#data-quality-issues",
    "href": "posts/2023-10-31-learning_curves/index.html#data-quality-issues",
    "title": "Learning Curves: Experenice vs Performance",
    "section": "7. Data Quality Issues",
    "text": "7. Data Quality Issues\n\nDiagnosis: The learning curves do not show improvement, or the validation performance is significantly lower than training performance.\nFollow-up Actions:\n\nClean the dataset to remove noisy examples and outliers.\nCheck for and correct any mislabeled examples.\nEnsure that the features are appropriately scaled and encoded."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html",
    "href": "posts/2023-10-31-model_diagnostics/index.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "I am at a point where whenever someone mentions modeling, I always get goosebumps for some reason. And when they start saying “You know, data is the new gold!”, it’s just too much for me. Here’s my hot take: all jobs are data analyst job and all scientific disciplines are data science.\nThe massive hype surrounding quantitative methods often undermines my satirical take on modeling. I think that models are essentially a collection of assumptions and abstractions, constructed to simplify and represent complex real-world phenomena.No matter how “great” your “model” is, we are still in the information business fellas. Someone will have to adjust to make the model works."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#model-fit",
    "href": "posts/2023-10-31-model_diagnostics/index.html#model-fit",
    "title": "Model Diagnostics",
    "section": "1. Model Fit:",
    "text": "1. Model Fit:\n\nGoodness of Fit: Assess how well the model’s predictions match the observed data. For regression models, this might involve R², adjusted R², or residual plots. For classification models, you might use accuracy, precision, recall, or ROC curves.\nResidual Analysis: Analyze the residuals (the difference between observed and predicted values) to check for patterns, heteroscedasticity (non-constant variance), and normality. Tools include residual plots, QQ-plots, and statistical tests."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#model-performance",
    "href": "posts/2023-10-31-model_diagnostics/index.html#model-performance",
    "title": "Model Diagnostics",
    "section": "2. Model Performance:",
    "text": "2. Model Performance:\n\nCross-Validation: Use techniques like k-fold cross-validation to assess how well the model is likely to perform on unseen data.\nPerformance Metrics: Depending on the type of model (e.g., regression, classification), use appropriate metrics like Mean Squared Error (MSE), accuracy, F1 score, AUC-ROC, etc."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#overfitting-and-underfitting",
    "href": "posts/2023-10-31-model_diagnostics/index.html#overfitting-and-underfitting",
    "title": "Model Diagnostics",
    "section": "3. Overfitting and Underfitting:",
    "text": "3. Overfitting and Underfitting:\n\nLearning Curves: Plot training and validation error over time or against model complexity to diagnose overfitting (high variance) or underfitting (high bias).\nRegularization: Apply techniques like L1 or L2 regularization and tune their parameters to prevent overfitting."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#feature-relevance-and-importance",
    "href": "posts/2023-10-31-model_diagnostics/index.html#feature-relevance-and-importance",
    "title": "Model Diagnostics",
    "section": "4. Feature Relevance and Importance:",
    "text": "4. Feature Relevance and Importance:\n\nVariable Selection: Use techniques like forward selection, backward elimination, or regularization paths to identify the most important features.\nImportance Scores: For tree-based models (e.g., random forests, gradient boosting), look at feature importance scores."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#model-assumptions-and-conditions",
    "href": "posts/2023-10-31-model_diagnostics/index.html#model-assumptions-and-conditions",
    "title": "Model Diagnostics",
    "section": "5. Model Assumptions and Conditions:",
    "text": "5. Model Assumptions and Conditions:\n\nLinearity: For linear models, check that the relationship between predictors and the target is linear.\nIndependence: Ensure that observations are independent of each other.\nHomoscedasticity: The variance of residuals should be constant across all levels of the independent variables.\nNormality of Errors: For certain types of models (e.g., linear regression), the residuals should be normally distributed."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#outliers-and-leverage-points",
    "href": "posts/2023-10-31-model_diagnostics/index.html#outliers-and-leverage-points",
    "title": "Model Diagnostics",
    "section": "6. Outliers and Leverage Points:",
    "text": "6. Outliers and Leverage Points:\n\nOutlier Detection: Identify and assess the impact of outliers on the model.\nInfluence Points: Use measures like Cook’s distance to identify points that have a disproportionate influence on the model fit."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#model-complexity",
    "href": "posts/2023-10-31-model_diagnostics/index.html#model-complexity",
    "title": "Model Diagnostics",
    "section": "7. Model Complexity:",
    "text": "7. Model Complexity:\n\nSimplicity vs. Complexity: Balance the complexity of the model to ensure it captures the underlying patterns without being overly complex.\nDimensionality Reduction: Apply techniques like PCA if the model is suffering from the curse of dimensionality."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#comparison-with-other-models",
    "href": "posts/2023-10-31-model_diagnostics/index.html#comparison-with-other-models",
    "title": "Model Diagnostics",
    "section": "8. Comparison with Other Models:",
    "text": "8. Comparison with Other Models:\n\nModel Selection: Compare the performance, interpretability, and complexity of different models to choose the best one."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#interpretability-and-explainability",
    "href": "posts/2023-10-31-model_diagnostics/index.html#interpretability-and-explainability",
    "title": "Model Diagnostics",
    "section": "9. Interpretability and Explainability:",
    "text": "9. Interpretability and Explainability:\n\nModel Explanation: Use tools and techniques to explain the predictions of complex models, especially for high-stakes decisions."
  },
  {
    "objectID": "posts/2023-10-31-model_diagnostics/index.html#validation-on-different-data-subsets",
    "href": "posts/2023-10-31-model_diagnostics/index.html#validation-on-different-data-subsets",
    "title": "Model Diagnostics",
    "section": "10. Validation on Different Data Subsets:",
    "text": "10. Validation on Different Data Subsets:\n\nStratified Sampling: Ensure that the model performs well across different strata or subsets of the data."
  },
  {
    "objectID": "posts/2023-11-02-model_assumptions/index.html",
    "href": "posts/2023-11-02-model_assumptions/index.html",
    "title": "Violations of Linear Regression Assumptions",
    "section": "",
    "text": "Let’s consider a marketing example where you are trying to predict sales based on different types of advertising budgets (e.g., TV, radio, online). Here’s how you can address violations of regression assumptions with this data:\n\nLinearity\n\nValidation: Plot residuals vs. fitted values. A non-random pattern suggests non-linearity.\nSymptom: The relationship between advertising budgets and sales is not linear.\nImplication: If doubling the TV advertising budget does not double the sales, a linear model might not be appropriate. If the relationship between predictors and the response variable is not linear, the predictions can be biased, especially for values outside the range of the observed data.\nWhat to Do:\n\nTransform the Data: Apply transformations like log, square root, or polynomial transformations.\nAdd Polynomial Terms: Include squared or cubic terms of predictors.\nUse Non-linear Models: Decision trees, neural networks, or other non-linear models might be more appropriate.\n\n\n\n\nIndependence\n\nValidation: Durbin-Watson test (values close to 2 suggest independence). Check if the residuals are independent\nSymptom: Observations are not independent.\nImplication: The sales result from one advertising campaign does not depend on the sales result from another campaign. For example, a successful campaign last month does not necessarily mean this month’s campaign will also be successful. If observations are not independent, the predictions might still be unbiased, but the standard errors of the coefficient estimates could be underestimated, leading to overly optimistic significance tests. This can be a serious issue if the goal is inference.\nWhat to Do:\n\nAdd Missing Variables: Include variables that account for the observed patterns.\nUse Time Series Models: If data is time-dependent, models like ARIMA might be appropriate.\nAdjust for Clusters: If data is clustered, use models that account for this.\n\n\n\n\nHomoscedasticity\n\nValidation: Look for a constant spread in a plot of residuals vs. fitted values. Use the Breusch-Pagan test, or Goldfeld-Quandt test.\nSymptom: The variance of residuals is not constant across all levels of advertising budgets.\nImplication: Whether you spend a small or large amount on advertising, the variability in sales remains consistent. A higher budget does not lead to more variability in sales predictions. If the variance of the errors is not constant, the model might give too much weight to some observations, leading to inefficient estimates. Predictions might still be accurate on average, but the uncertainty around the predictions could be misestimated.\nWhat to Do:\n\nTransform the Data: Log or square root transformations of the response variable.\nUse Weighted Regression: Assign different weights to observations.\n\n\n\n\nNormality of Errors\n\nValidation: QQ-plot should show residuals following a straight line. Shapiro-Wilk test can be used for formal testing.\nSymptom: Residuals are not normally distributed.\nImplication: If residuals are skewed, the model might consistently overestimate or underestimate sales for certain budget levels. This assumption is mainly important for hypothesis testing and constructing confidence intervals. If errors are not normally distributed, predictions might still be unbiased, but tests and intervals could be inaccurate.\nWhat to Do:\n\nTransform the Data: Applying a transformation might help.\nIncrease Sample Size: Larger samples tend to have more normally distributed means.\n\n\n\n\nSummary\nFor Prediction: If the primary goal is prediction, slight violations of assumptions might not be as critical, especially if the model’s predictive performance is validated using external datasets or cross-validation. However, major violations should still be addressed to ensure reliable predictions.\nFor Inference: If the goal is to understand the relationship between variables or make policy decisions, it’s crucial that the model meets the assumptions. Violations can lead to biased or incorrect conclusions.\nIn a business setting of predicting sales from advertising budget using a linear regression model, the data should ideally exhibit a straight-line relationship (Linearity), the sales outcomes should be independent of each other (Independence), the variance in sales should be consistent across all budget levels (Homoscedasticity), and the deviations from the predicted sales should follow a normal distribution (Normality of Errors)."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "“Blog”",
    "section": "",
    "text": "This blog is inspired by the concept of out-of-core learning, which refers to the ability to learn from data that cannot fit into a computer’s memory (RAM) at once. While building indexes, which is similar to creating shortcuts or directories for quick access to information, the computer can access specific pieces of information.\nComparing this concept to human learning, where the brain is considered the RAM, I’d like to be better at how to ask good questions on interesting topics. These questions will essentially be queries to the Large Language Models such as OpenAI’s GPT 4.0, and I’ll use them as my mental indexes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference Case Study: Disease Prevalence\n\n\n\nBayesian\n\n\n\ntbd\n\n\n\nNet Zhang\n\n\nJun 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViolations of Linear Regression Assumptions\n\n\n\nR\n\n\nML\n\n\nModel Diagnostics\n\n\n\nThis post discusses how to validate linear model assumptions, the implication of the violation, and follow-up actions.\n\n\n\nNet Zhang\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Curves: Experenice vs Performance\n\n\n\nR\n\n\nML\n\n\nLearning Curves\n\n\nModel Diagnostics\n\n\n\nThis post discuss how to use learning curves as a model diagnostic tool\n\n\n\nNet Zhang\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\n\n\n\nR\n\n\nML\n\n\nModel Diagnostics\n\n\n\nYou built a model. Now what?\n\n\n\nNet Zhang\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download Current Resume"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "",
    "text": "In Bayesian statistics, we use a powerful framework to update our beliefs based on new evidence. Unlike traditional frequentist methods, Bayesian approaches take into account prior information and produce a posterior distribution that reflects updated beliefs after observing the data. In this tutorial, we will walk through a simple Bayesian modeling process using regional disease testing as a case study.\nWhat is this post about?\nThis is part of my Bayesian Inference series, where I present problems (mostly in public health settings) and apply Bayesian approaches.\nWho is this for? And what’s the motivation?\nI’ve been working as a data scientist at a higher institution for the past 2 years, supporting data operations from public health authorities and researchers. However, I’ve noticed that my job has started to shift more towards data management and software engineering but I don’t want my modeling skills to get rusty. Working at a university has its perks, such as many opportunities to teach, and I’ve learned a lot from participating in workshops. I believe that mastering knowledge doesn’t always mean being able to pass it on. For this reason, I want to create theory-focused tutorials with R or Python scripts that follow good software engineering principles. I want to help people, including myself, to master the theory behind basic Bayesian models. These tutorials are aimed at those who are passionate about Bayesian Inference and would like to refresh their knowledge of Bayesian 101 Modeling."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#assumptions",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#assumptions",
    "title": "Bayesian Inference Case Study: Disease Case Rate",
    "section": "Assumptions",
    "text": "Assumptions\nTo apply the Bayesian method, we start with a set of assumptions:\n\nBinary Outcomes: Each test result is either positive or negative.\nIndependence: Each test result is assumed to be independent of others.\nIdentical Distribution: Each individual in the population has the same probability of testing positive."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#build-a-model",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#build-a-model",
    "title": "Bayesian Inference Case Study: Disease Case Rate",
    "section": "Build A Model",
    "text": "Build A Model\nGiven these assumptions, we can treat the tests as independent and identically distributed random variables \\(x_i\\) following the Bernoulli distribution. The total number of positive cases \\(X\\) is a random variable that follows the binomial distribution.\n\\[\nx_i \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta)\n\\]\n\\[\nX \\sim \\text{Binomial}(n, \\theta)\n\\]\nAt this point, we have already specified our model. Now is the time to ask ourselves what kind of questions we wish our model to answer. In the public health setting, we are interested in questions like\n\nWhat is the estimated prevalence of the disease in the community?\nHow confident are we in our estimate of the disease prevalence?\nHow does new data (e.g., additional test results) update our beliefs about disease prevalence?\nWhat is the probability that the prevalence exceeds a certain threshold (e.g., 5%)?\nHow do different prior beliefs about the prevalence affect our posterior estimates?\nGiven our current estimates, what is the expected number of positive cases in the next 50 tests?\nHow reliable are our predictions about the number of positive cases in the upcoming tests?\n\nIn summary, the typical expectation is to deliver an estimate and communicate the level of confidence in that estimation. You’ll see that once we establish the statistical distribution, addressing these questions will become more straightforward."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#distributions",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#distributions",
    "title": "Bayesian Inference Case Study: Disease Case Rate",
    "section": "Distributions",
    "text": "Distributions\n\nLikelihood Function\nThe likelihood function represents the probability of observing the data given a specific value of the parameter. Since the random variable \\(X\\) follows a binomial distribution, we can write the likelihood as a function of the data given the value of parameters as below:\n\\[\nf(X = x | \\theta) = \\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n\\]\n\n\nPrior Distribution\nThe prior distribution represents our initial beliefs about the parameter before any data is observed. The Beta distribution is often chosen as the prior distribution in Bayesian modeling for parameters that represent probabilities.\n\\[\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\nSince Beta distribution is a known distribution, we could write out its probability density function as follows:\n\\[\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\cdot \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\\[\n\\Gamma(\\alpha) = (\\alpha - 1)!\n\\]\n\\[\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n\\]\nWhy choose Beta as our prior?\nHere, I will briefly explain why we want to use Beta distribution as our prior in this example:\n\nFlexibility & Interpretability\n\nThe beta distribution is highly flexible and can take on a variety of shapes depending on its two parameters, \\(\\alpha\\) and \\(\\beta\\). This flexibility allows it to represent a wide range of prior beliefs about the probability parameter, in our case, the case rate.\nThe parameters of the Beta distribution have intuitive interpretations. \\(\\alpha\\) can be thought of as the number of prior successes, and \\(\\beta\\) is the number of prior failures. This makes it easy to incorporate prior knowledge or expert opinions. When we have complete uncertainty of the parameters, we assign the prior to \\(Beta(1,1)\\), which is equivalent to a uniform distribution.\n\nConjugate Prior for the Binomial Distribution\n\nWhat is conjugacy? The Beta distribution is the conjugate prior to the Binomial distribution. This means that when a Beta prior is combined with a Binomial likelihood, the posterior distribution is also a Beta distribution. You can see that this property greatly simplifies the Bayesian updating process when we derive the posterior for the unknown parameters.\nHow do we translate expert knowledge into hyper-parameters?\nGenerally, your expert team will provide an expected estimate and their confidence level.\nWe could derive the expectation and standard deviation from the Beta distribution likelihood.\n\\[\nE(\\theta)=\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nThe concentration of the Beta distribution is determined by \\(\\alpha+\\beta\\) A higher sum means more confidence.\nIn this case study, suppose our public health workers have an estimate of the prevalence rate to be 20% before we roll out the community testing. They have moderate confidence, similar to having results from about 50 prior tests with 10 positives. Therefore, we could set \\(\\alpha\\) to 10 and \\(\\beta\\) to 40.\n\n\nPosterior Distribution\nThe posterior distribution combines the prior distribution and the likelihood function using Bayes’ theorem. It represents our updated beliefs about the parameter after observing the data.\n\\[\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{f(x)}\n\\]\n\\[\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta}\n\\]\nThe denominator \\(\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta\\) is the normalizing constant ensuring that the posterior distribution integrates to 1. We can see that the posterior distribution is proportional to the product of the prior and the likelihood\n\\[\nf(\\theta | x) \\propto f(x | \\theta) \\cdot f(\\theta)\n\\]\n\\[\nf(\\theta | x) \\propto \\theta^{\\alpha + x - 1} \\cdot (1 - \\theta)^{\\beta + n - x - 1} \\\n\\]\n\\[\nf(\\theta | x) \\propto \\text{Beta}(\\alpha + x, \\beta + n - x)\n\\]\n\\[\n\\mathbb{E}(\\theta | x) = \\frac{\\alpha + x}{\\alpha + \\beta + n}\n\\]\n\n\nPredictive Distribution"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#likelihood-function",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#likelihood-function",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function represents the probability of observing the data given a specific value of the parameter. Since the random variable \\(X\\) follows a binomial distribution, we can write the likelihood as a function of the data given the value of parameters as below:\n\\[\nf(X = x | \\theta) = \\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n\\]"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#prior-distribution",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#prior-distribution",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Prior Distribution",
    "text": "Prior Distribution\nThe prior distribution represents our initial beliefs about the parameter before any data is observed. The Beta distribution is often chosen as the prior distribution in Bayesian modeling for parameters that represent probabilities.\n\\[\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\nSince Beta distribution is a known distribution, we could write out its probability density function as follows:\n\\[\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\cdot \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\\[\n\\Gamma(\\alpha) = (\\alpha - 1)!\n\\]\n\\[\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n\\]\nWhy choose Beta as our prior?\nHere, I will briefly explain why we want to use Beta distribution as our prior in this example:\n\nFlexibility & Interpretability\n\nThe beta distribution is highly flexible and can take on a variety of shapes depending on its two parameters, \\(\\alpha\\) and \\(\\beta\\). This flexibility allows it to represent a wide range of prior beliefs about the probability parameter, in our case, the case rate.\nThe parameters of the Beta distribution have intuitive interpretations. \\(\\alpha\\) can be thought of as the number of prior successes, and \\(\\beta\\) is the number of prior failures. This makes it easy to incorporate prior knowledge or expert opinions. When we have complete uncertainty of the parameters, we assign the prior to \\(Beta(1,1)\\), which is equivalent to a uniform distribution.\n\nConjugate Prior for the Binomial Distribution\n\nWhat is conjugacy? The Beta distribution is the conjugate prior to the Binomial distribution. This means that when a Beta prior is combined with a Binomial likelihood, the posterior distribution is also a Beta distribution. You can see that this property greatly simplifies the Bayesian updating process when we derive the posterior for the unknown parameters.\nHow do we translate expert knowledge into hyper-parameters?\nGenerally, your expert team will provide an expected estimate and their confidence level.\nWe could derive the expectation and standard deviation from the Beta distribution likelihood.\n\\[\nE(\\theta)=\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nThe concentration of the Beta distribution is determined by \\(\\alpha+\\beta\\) A higher sum means more confidence.\nIn this case study, suppose our public health workers have an estimate of the prevalence rate to be 20% before we roll out the community testing. They have moderate confidence, similar to having results from about 50 prior tests with 10 positives. Therefore, we could set \\(\\alpha\\) to 10 and \\(\\beta\\) to 40."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#posterior-distribution",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#posterior-distribution",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nThe posterior distribution combines the prior distribution and the likelihood function using Bayes’ theorem. It represents our updated beliefs about the parameter after observing the data.\n\\[\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{f(x)}\n\\]\n\\[\nf(\\theta | x) = \\frac{f(x | \\theta) \\cdot f(\\theta)}{\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta}\n\\]\nThe denominator \\(\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta\\) is the normalizing constant ensuring that the posterior distribution integrates to 1. We can see that the posterior distribution is proportional to the product of the prior and the likelihood\n\\[\nf(\\theta | x) \\propto f(x | \\theta) \\cdot f(\\theta)\n\\]\n\\[\nf(\\theta | x) \\propto \\theta^{\\alpha + x - 1} \\cdot (1 - \\theta)^{\\beta + n - x - 1} \\\n\\]\n\\[\nf(\\theta | x) \\propto \\text{Beta}(\\alpha + x, \\beta + n - x)\n\\]\n\\[\n\\mathbb{E}(\\theta | x) = \\frac{\\alpha + x}{\\alpha + \\beta + n}\n\\]"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#predictive-distribution",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#predictive-distribution",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Predictive Distribution",
    "text": "Predictive Distribution\nThe predictive distribution allows us to make predictions about future observations based on the posterior distribution. It incorporates the uncertainty in the parameter estimate.\nFrom the previous notes, we have the prior PDF and likelihood function\n\\[\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\cdot \\theta^{x} (1 - \\theta)^{n - x}\n\\]\n\\[\nf(x|\\theta)=\\binom{n}{x} \\theta^x (1 - \\theta)^{n - x}\n\\]\nAnd we could derive the predictive distribution using the prior and likelihood:\n\\[\nf(x)=\\int_0^1 f(x | \\theta) \\cdot f(\\theta) d\\theta\n\\]\n\\[\nf(x)=\\binom{n}{x}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\frac{\\Gamma(\\alpha + x)\\Gamma(n+\\beta-x)}{\\Gamma(\\alpha+\\beta+n)}=\\binom{n}{x}\\frac{B(\\alpha+x, n+\\beta-x)}{B(\\alpha, \\beta)}\n\\]\nGiven a Beta prior, the predictive distribution is a Beta-Binomial Distribution modeling the number of successes in 𝑛 trials when the probability of success \\(\\theta\\) is uncertain.\n\\[\nP(X=x|\\alpha, \\beta,n) = \\binom{n}{x}\\frac{B(\\alpha+x, n+\\beta-x)}{B(\\alpha, \\beta)}\n\\]\nThe expected value of the Beta-binomial distribution is given by:\n\\[\n\\mathbb{E}[X|\\alpha, \\beta,n] = \\mathbb{E}[n*p] = n \\cdot \\frac{\\alpha}{\\alpha + \\beta}\n\\]"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#estimating-disease-prevalence",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#estimating-disease-prevalence",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Estimating Disease Prevalence",
    "text": "Estimating Disease Prevalence\nWhat is the estimated prevalence of the disease in the community?\nHow confident are we in our estimate of the disease prevalence?\nWhat is the probability that the prevalence exceeds a certain threshold (e.g., 5%)?\nThese are questions about our estimation of the unknown parameters. We could answer these questions using the posterior distribution’s probability density function.\nFirst, plug in the data and hyper-parameters into the posterior function we get\n\\[\nf(\\theta | x) \\propto \\text{Beta}(32, 128)\n\\]\nThen let’s plot out the distribution\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\n\n# Function to calculate posterior parameters\nget_post_params &lt;- function(data, prior) {\n  post_alpha &lt;- prior$alpha + data$x\n  post_beta &lt;- prior$beta + data$n - data$x\n  return(list(alpha = post_alpha, beta = post_beta))\n}\n\n# Function to generate posterior data for plotting\ngenerate_post_pdf_tbl &lt;- function(post_params, label) {\n  tibble(\n    theta = seq(0, 1, length.out = 5000),\n    posterior_density = dbeta(theta, post_params$alpha, post_params$beta),\n    round = label\n  )\n}\n\n# Function to calculate credible interval and mean\nget_credible_interval_and_mean &lt;- function(post_params) {\n  credible_interval &lt;- qbeta(c(0.025, 0.975), post_params$alpha, post_params$beta)\n  mean_posterior &lt;- post_params$alpha / (post_params$alpha + post_params$beta)\n  return(list(credible_interval = credible_interval, mean_posterior = mean_posterior))\n}\n\n# Data for the first round\ndata_round_1 &lt;- list(n = 100, x = 22)\nprior_params &lt;- list(alpha = 10, beta = 40)\n\n# Calculate posterior for the first round\npost_params_1 &lt;- get_post_params(data_round_1, prior_params)\npost_pdf_tbl_1 &lt;- generate_post_pdf_tbl(post_params_1, \"Posterior after Round 1\")\ncredible_info_1 &lt;- get_credible_interval_and_mean(post_params_1)\n\n# Plot the posterior distribution for the first round\nggplot(post_pdf_tbl_1, aes(x = theta, y = posterior_density)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = credible_info_1$mean_posterior, linetype = \"dashed\", color = \"red\") +\n  geom_ribbon(data = filter(post_pdf_tbl_1, theta &gt;= credible_info_1$credible_interval[1] & theta &lt;= credible_info_1$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"blue\", alpha = 0.2) +\n  labs(title = \"Posterior Distribution after Round 1\",\n       x = \"Disease Prevalence (Theta)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = credible_info_1$mean_posterior, y = max(post_pdf_tbl_1$posterior_density) * 0.9, label = paste(\"Mean:\", round(credible_info_1$mean_posterior, 3)), color = \"red\")\n\n\n\n\nBased on our posterior, we estimate the disease prevalence with the posterior expectation value \\(E(\\theta|X=x)=0.213\\). Our confidence is reflected by the spread of the posterior distribution. The standard deviation and the 95% credible interval quantify this confidence."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#updating-beliefs-with-new-data",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#updating-beliefs-with-new-data",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Updating Beliefs with New Data",
    "text": "Updating Beliefs with New Data\nOne of the core strengths of Bayesian inference lies in its ability to update our beliefs as new data becomes available. Now, suppose we received a new round of testing results: 100 individuals were tested, and 8 tested positive.\nOur posterior from the last round now becomes the prior, and we’ll use the new test data to update the posterior:\n\\[\nf(\\theta | x) \\propto \\text{Beta}(32 + x, 128 + n - x)\n\\]\n\\[\nf(\\theta | x) \\propto \\text{Beta}(40, 220)\n\\]\nA pretty easy process. And let’s visualize the updated posterior function:\n\n# Given posterior for the second round\npost_params_2 &lt;- list(alpha = 40, beta = 220)\npost_pdf_tbl_2 &lt;- generate_post_pdf_tbl(post_params_2, \"Posterior after Round 2\")\ncredible_info_2 &lt;- get_credible_interval_and_mean(post_params_2)\n\n# Combine both posteriors into one data frame\npost_pdf_tbl &lt;- bind_rows(post_pdf_tbl_1, post_pdf_tbl_2)\n\n# Plot both posterior distributions\nggplot(post_pdf_tbl, aes(x = theta, y = posterior_density, color = round, fill = round)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = credible_info_1$mean_posterior, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = credible_info_2$mean_posterior, linetype = \"dashed\", color = \"blue\") +\n  geom_ribbon(data = filter(post_pdf_tbl_1, theta &gt;= credible_info_1$credible_interval[1] & theta &lt;= credible_info_1$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"blue\", alpha = 0.2) +\n  geom_ribbon(data = filter(post_pdf_tbl_2, theta &gt;= credible_info_2$credible_interval[1] & theta &lt;= credible_info_2$credible_interval[2]), \n              aes(ymax = posterior_density), ymin = 0, fill = \"green\", alpha = 0.2) +\n  labs(title = \"Posterior Distributions of Disease Prevalence\",\n       x = \"Disease Prevalence (Theta)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = credible_info_1$mean_posterior, y = max(post_pdf_tbl_1$posterior_density) * 0.9, label = paste(\"Mean Round 1:\", round(credible_info_1$mean_posterior, 3)), color = \"red\") +\n  annotate(\"text\", x = credible_info_2$mean_posterior, y = max(post_pdf_tbl_2$posterior_density) * 0.9, label = paste(\"Mean Round 2:\", round(credible_info_2$mean_posterior, 3)), color = \"blue\")"
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#predictive-analysis",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#predictive-analysis",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Predictive Analysis",
    "text": "Predictive Analysis\nGiven our current estimates, what is the expected number of positive cases in the next 50 tests?\nHow reliable are our predictions about the number of positive cases in the upcoming tests?\nWe need to have the predictive distribution at hand to answer these types of questions. And luckily, from the previous section, we’ve already proven that the predictive distribution follows a Beta-Binomial distribution, which is a function of the data given the prior.\n\\[\nP(X=x|\\alpha_{post}, \\beta_{post},n) = \\binom{n}{x}\\frac{B(\\alpha_{post}+x, n+\\beta_{post}-x)}{B(\\alpha_{post}, \\beta_{post})}\n\\]\nBased on two rounds of testing data and the question statement, we have \\(\\alpha_{post} = 40\\), \\(\\beta_{post} = 220\\), and \\(n=50\\).\nThere is currently no base R function for beta-binomial distribution. However, you could try implementing it independently since we already figured out the math.\n\n# Function to calculate Beta-binomial probability\ndbeta_binom &lt;- function(x, n, alpha, beta) {\n  choose(n, x) * beta(alpha + x, beta + n - x) / beta(alpha, beta)\n}\n\n# Function to calculate Beta-binomial quantiles\nqbeta_binom &lt;- function(p, n, alpha, beta) {\n  sapply(p, function(prob) {\n    q &lt;- 0\n    cumulative_prob &lt;- 0\n    while (cumulative_prob &lt; prob) {\n      cumulative_prob &lt;- cumulative_prob + dbeta_binom(q, n, alpha, beta)\n      q &lt;- q + 1\n    }\n    return(q - 1)\n  })\n}\n\nNow let’s try to visualize the predictive distribution\n\n# Predictive distribution parameters\nn_future &lt;- 50  # Number of future trials\nalpha_post &lt;- post_params_2$alpha\nbeta_post &lt;- post_params_2$beta\n\n# Generate the predictive distribution using Beta-binomial\npred_x &lt;- 0:n_future\npred_density &lt;- sapply(pred_x, function(x) dbeta_binom(x, n_future, alpha_post, beta_post))\n\n# Calculate the 95% predictive interval for the predictive distribution\npredictive_interval &lt;- qbeta_binom(c(0.025, 0.975), n_future, alpha_post, beta_post)\n\n# Create a data frame for the predictive distribution\npred_df &lt;- tibble(\n  x = pred_x,\n  density = pred_density\n)\n\n# Plot the predictive distribution\nggplot(pred_df, aes(x = x, y = density)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_errorbar(aes(x = predictive_interval[1], ymin = 0, ymax = max(density)), linetype = \"dotted\", color = \"blue\", width = 0.2) +\n  geom_errorbar(aes(x = predictive_interval[2], ymin = 0, ymax = max(density)), linetype = \"dotted\", color = \"blue\", width = 0.2) +\n  labs(title = \"Predictive Distribution of Future Positive Tests\",\n       x = \"Number of Positive Tests\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  # annotate(\"text\", x = alpha_post / (alpha_post + beta_post) * n_future, y = max(density) * 0.9, \n  #          label = paste(\"Mean:\", round(alpha_post / (alpha_post + beta_post) * n_future, 2)), color = \"red\") +\n  geom_ribbon(data = filter(pred_df, x &gt;= predictive_interval[1] & x &lt;= predictive_interval[2]), \n              aes(ymax = density), ymin = 0, fill = \"blue\", alpha = 0.2)\n\n\n\n\nTherefore, given our current estimates, we expect approximately 8 positive cases in the next 50 tests. Based on the 95% predictive interval (approximately 4 to 12), our predictions are reliable within this range, indicating that while there’s some uncertainty, we have a high probability that the true count will fall within these bounds."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#cluster-infections",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#cluster-infections",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Cluster Infections",
    "text": "Cluster Infections\nIn a real-world scenario, infections might not be evenly distributed across the population. There could be clusters of infections due to localized outbreaks or hot-pots, leading to correlations between test results.\nTo address this, we should begin with identify and model clusters within the data. For example, use hierarchical models that allow for different prevalence rates in different subgroups or locations."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#testing-bias",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#testing-bias",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Testing Bias",
    "text": "Testing Bias\nThe way individuals are selected for testing might not be random. For example, people with symptoms or known exposure to the disease might be more likely to get tested, which can skew the results.\nIf you are from the ML world, you already know what to do. We could include covariates that account for factors affecting the probability of a positive test, such as symptoms, exposure history, or demographic factors."
  },
  {
    "objectID": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#temporal-changes",
    "href": "posts/2024-06-12-Bayesian_Binomial_Disease_Rate/index.html#temporal-changes",
    "title": "Bayesian Inference Case Study: Disease Prevalence",
    "section": "Temporal Changes",
    "text": "Temporal Changes\nAs the title suggested, the prevalence of the disease might change over time due to factors such as public health interventions, changes in behavior, or natural progression of the outbreak. This means the probability of a positive test could vary over time. We should consider using time series models or Bayesian hierarchical models that account for changes in prevalence over time."
  }
]